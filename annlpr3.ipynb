{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejas9523/ANNLAB/blob/main/annlpr3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEG8CQK7UpLt"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, input_size):\n",
        "        self.weights = np.random.rand(input_size)\n",
        "        self.bias = np.random.rand()\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        summation = np.dot(inputs, self.weights) + self.bias\n",
        "        return 1 if summation > 0 else 0"
      ],
      "metadata": {
        "id": "a9KhdVIDUrJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_binary_ascii(number):\n",
        "    binary_ascii = format(number, '08b')\n",
        "    return [int(bit) for bit in binary_ascii]"
      ],
      "metadata": {
        "id": "aIdMOy9vUxop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_perceptron():\n",
        "    training_data = [\n",
        "        (0, 0),\n",
        "        (1, 1),\n",
        "        (2, 0),\n",
        "        (3, 1),\n",
        "        (4, 0),\n",
        "        (5, 1),\n",
        "        (6, 0),\n",
        "        (7, 1),\n",
        "        (8, 0),\n",
        "        (9, 1),\n",
        "    ]\n",
        "\n",
        "    perceptron = Perceptron(input_size=8)\n",
        "\n",
        "    epochs = 1000\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for number, label in training_data:\n",
        "            binary_ascii = convert_to_binary_ascii(number)\n",
        "            prediction = perceptron.predict(binary_ascii)\n",
        "            error = label - prediction\n",
        "\n",
        "            perceptron.weights += learning_rate * error * np.array(binary_ascii)\n",
        "            perceptron.bias += learning_rate * error\n",
        "\n",
        "    return perceptron"
      ],
      "metadata": {
        "id": "zYdYF90cU0E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    perceptron = train_perceptron()\n",
        "\n",
        "    test_numbers = [2, 7, 4, 9, 1]\n",
        "\n",
        "    for number in test_numbers:\n",
        "        binary_ascii = convert_to_binary_ascii(number)\n",
        "        prediction = perceptron.predict(binary_ascii)\n",
        "\n",
        "        print(f\"Number: {number}, Prediction: {'Even' if prediction == 0 else 'Odd'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ5efHOyU_T5",
        "outputId": "174ec648-a8a2-4664-ef62-63d9e9946dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number: 2, Prediction: Even\n",
            "Number: 7, Prediction: Odd\n",
            "Number: 4, Prediction: Even\n",
            "Number: 9, Prediction: Odd\n",
            "Number: 1, Prediction: Odd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the training data (ASCII representation of digits 0 to 9)\n",
        "training_data = np.array([\n",
        "    [48, 48, 48, 48, 48, 48, 48, 48, 48],  # 0\n",
        "    [49, 48, 48, 48, 48, 48, 48, 48, 48],  # 1\n",
        "    [49, 49, 48, 48, 48, 48, 48, 48, 48],  # 2\n",
        "    [49, 49, 49, 48, 48, 48, 48, 48, 48],  # 3\n",
        "    [49, 49, 49, 49, 48, 48, 48, 48, 48],  # 4\n",
        "    [49, 49, 49, 49, 49, 48, 48, 48, 48],  # 5\n",
        "    [49, 49, 49, 49, 49, 49, 48, 48, 48],  # 6\n",
        "    [49, 49, 49, 49, 49, 49, 49, 48, 48],  # 7\n",
        "    [49, 49, 49, 49, 49, 49, 49, 49, 48],  # 8\n",
        "    [49, 49, 49, 49, 49, 49, 49, 49, 49]   # 9\n",
        "])\n",
        "\n",
        "# Define the target labels (0 for even, 1 for odd)\n",
        "target_labels = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
        "        self.W = np.zeros(input_size + 1)\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "\n",
        "    def activation_fn(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.insert(x, 0, 1)  # Bias term\n",
        "        z = self.W.T.dot(x)\n",
        "        a = self.activation_fn(z)\n",
        "        return a\n",
        "\n",
        "    def train(self, X, d):\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(d.shape[0]):\n",
        "                y = self.predict(X[i])\n",
        "                e = d[i] - y\n",
        "                self.W = self.W + self.lr * e * np.insert(X[i], 0, 1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_size = 9  # Size of the input vector (ASCII representation of a digit)\n",
        "    perceptron = Perceptron(input_size)\n",
        "    perceptron.train(training_data, target_labels)\n",
        "\n",
        "    # Test the trained perceptron\n",
        "    test_cases = [\n",
        "        [48, 48, 48, 48, 48, 48, 48, 48, 48],  # 0\n",
        "        [49, 48, 48, 48, 48, 48, 48, 48, 48],  # 1\n",
        "        [49, 49, 48, 48, 48, 48, 48, 48, 48],  # 2\n",
        "        [49, 49, 49, 48, 48, 48, 48, 48, 48],  # 3\n",
        "        [49, 49, 49, 49, 48, 48, 48, 48, 48],  # 4\n",
        "        [49, 49, 49, 49, 49, 48, 48, 48, 48],  # 5\n",
        "        [49, 49, 49, 49, 49, 49, 48, 48, 48],  # 6\n",
        "        [49, 49, 49, 49, 49, 49, 49, 48, 48],  # 7\n",
        "        [49, 49, 49, 49, 49, 49, 49, 49, 48],  # 8\n",
        "        [49, 49, 49, 49, 49, 49, 49, 49, 49]   # 9\n",
        "    ]\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        prediction = perceptron.predict(test_case)\n",
        "        digit = i\n",
        "        if prediction == 0:\n",
        "            result = \"even\"\n",
        "        else:\n",
        "            result = \"odd\"\n",
        "        print(f\"Digit {digit} is {result}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9aLizFOgNnp",
        "outputId": "a0762eaf-59d9-41aa-fd15-47ab52a76cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit 0 is odd.\n",
            "Digit 1 is odd.\n",
            "Digit 2 is odd.\n",
            "Digit 3 is odd.\n",
            "Digit 4 is odd.\n",
            "Digit 5 is odd.\n",
            "Digit 6 is odd.\n",
            "Digit 7 is odd.\n",
            "Digit 8 is odd.\n",
            "Digit 9 is odd.\n"
          ]
        }
      ]
    }
  ]
}